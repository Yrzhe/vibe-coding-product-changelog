#!/usr/bin/env python3
"""
ä½¿ç”¨ LLM ä¸ºåŠŸèƒ½æ›´æ–°æ‰“æ ‡
æ–°é€»è¾‘ï¼šLLM åªè¯†åˆ«äºŒçº§æ ‡ç­¾ï¼Œé€šè¿‡æ˜ å°„è¡¨è‡ªåŠ¨è·å¾—ä¸€çº§æ ‡ç­¾
æ”¯æŒç½‘ç»œé”™è¯¯å’Œ JSON è§£æé”™è¯¯é‡è¯•
"""

import json
import time
import re
from pathlib import Path
import requests


# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 2  # ç§’


def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆæ”¯æŒæœ¬åœ°å’Œ Docker ç¯å¢ƒï¼‰"""
    script_dir = Path(__file__).parent
    if script_dir == Path("/app"):
        return Path("/app")
    return script_dir.parent


def get_script_dir():
    """è·å–è„šæœ¬ç›®å½•"""
    script_dir = Path(__file__).parent
    if script_dir == Path("/app"):
        return Path("/app")
    return script_dir


def load_config():
    """åŠ è½½ LLM é…ç½®"""
    config_path = get_script_dir() / "prompts" / "llm_config.json"
    with open(config_path, "r", encoding="utf-8") as f:
        configs = json.load(f)
    return configs[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªé…ç½®


def load_tags():
    """åŠ è½½æ ‡ç­¾ä½“ç³»ï¼ˆæ–°ç»“æ„ï¼šprimary_tags + subtag_to_primaryï¼‰"""
    tags_path = get_project_root() / "info" / "tag.json"
    with open(tags_path, "r", encoding="utf-8") as f:
        return json.load(f)


def save_tags(tags_data: dict):
    """ä¿å­˜æ ‡ç­¾ä½“ç³»"""
    tags_path = get_project_root() / "info" / "tag.json"
    with open(tags_path, "w", encoding="utf-8") as f:
        json.dump(tags_data, f, ensure_ascii=False, indent=4)


def normalize_name(name: str) -> str:
    """æ ‡å‡†åŒ–åç§°ï¼Œç”¨äºæ¨¡ç³ŠåŒ¹é…"""
    return name.lower().strip().replace(" ", "").replace("-", "").replace("_", "")


def build_subtag_index(tags_data: dict) -> dict:
    """
    æ„å»ºäºŒçº§æ ‡ç­¾ç´¢å¼•ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾å’Œæ˜ å°„
    è¿”å›: {
        "subtag_norm_to_original": {"openai": "OpenAI", ...},
        "subtag_to_primary": {"OpenAI": "AI Model", ...},
        "all_subtags": ["OpenAI", "Anthropic", ...]
    }
    """
    subtag_to_primary = tags_data.get("subtag_to_primary", {})
    subtag_norm_to_original = {}
    all_subtags = []
    
    for subtag_name in subtag_to_primary.keys():
        norm = normalize_name(subtag_name)
        subtag_norm_to_original[norm] = subtag_name
        all_subtags.append(subtag_name)
    
    return {
        "subtag_norm_to_original": subtag_norm_to_original,
        "subtag_to_primary": subtag_to_primary,
        "all_subtags": all_subtags
    }


def normalize_subtag(subtag_name: str, subtag_index: dict) -> str:
    """
    æ ‡å‡†åŒ–äºŒçº§æ ‡ç­¾åç§°ï¼ˆä¿®æ­£ç©ºæ ¼ã€å¤§å°å†™å·®å¼‚ï¼‰
    è¿”å›: æ ‡å‡†åŒ–åçš„åç§°ï¼Œå¦‚æœæ˜¯æ–°æ ‡ç­¾åˆ™è¿”å›åŸåç§°
    """
    norm = normalize_name(subtag_name)
    norm_to_original = subtag_index.get("subtag_norm_to_original", {})
    
    if norm in norm_to_original:
        return norm_to_original[norm]
    return subtag_name


def map_subtags_to_tags(subtags: list, tags_data: dict, subtag_index: dict) -> list:
    """
    å°†äºŒçº§æ ‡ç­¾åˆ—è¡¨æ˜ å°„åˆ°å®Œæ•´çš„æ ‡ç­¾ç»“æ„
    è¾“å…¥: ["OpenAI", "Agent Mode", "Custom Domain"]
    è¾“å‡º: [
        {"name": "AI Model", "subtags": [{"name": "OpenAI"}]},
        {"name": "Agent", "subtags": [{"name": "Agent Mode"}]},
        {"name": "Deployment", "subtags": [{"name": "Custom Domain"}]}
    ]
    """
    subtag_to_primary = tags_data.get("subtag_to_primary", {})
    
    # è·å–æ‰€æœ‰ä¸€çº§æ ‡ç­¾åï¼ˆç”¨äºè¿‡æ»¤ LLM é”™è¯¯è¿”å›çš„ä¸€çº§æ ‡ç­¾åï¼‰
    primary_tag_names = {pt["name"] for pt in tags_data.get("primary_tags", [])}
    
    # æŒ‰ä¸€çº§æ ‡ç­¾åˆ†ç»„
    primary_to_subtags = {}
    new_subtags = []  # æ–°çš„äºŒçº§æ ‡ç­¾ï¼ˆéœ€è¦å½’å…¥ Othersï¼‰
    
    for subtag in subtags:
        # æ ‡å‡†åŒ–åç§°
        normalized = normalize_subtag(subtag, subtag_index)
        
        # è¿‡æ»¤æ‰ä¸€çº§æ ‡ç­¾åï¼ˆLLM é”™è¯¯è¿”å›ï¼‰
        if normalized in primary_tag_names:
            print(f"       âš ï¸ å¿½ç•¥ä¸€çº§æ ‡ç­¾å: {normalized}")
            continue
        
        if normalized in subtag_to_primary:
            primary = subtag_to_primary[normalized]
            if primary not in primary_to_subtags:
                primary_to_subtags[primary] = []
            primary_to_subtags[primary].append({"name": normalized})
        else:
            # æ–°çš„äºŒçº§æ ‡ç­¾ï¼Œå½’å…¥ Others
            new_subtags.append(normalized)
    
    # å¤„ç†æ–°çš„äºŒçº§æ ‡ç­¾ - å½’å…¥ Others
    if new_subtags:
        if "Others" not in primary_to_subtags:
            primary_to_subtags["Others"] = []
        for new_subtag in new_subtags:
            primary_to_subtags["Others"].append({"name": new_subtag})
            # æ›´æ–°æ˜ å°„è¡¨
            tags_data["subtag_to_primary"][new_subtag] = "Others"
            subtag_index["subtag_to_primary"][new_subtag] = "Others"
            subtag_index["subtag_norm_to_original"][normalize_name(new_subtag)] = new_subtag
            subtag_index["all_subtags"].append(new_subtag)
            # æ·»åŠ åˆ° Others çš„ subtags åˆ—è¡¨
            for pt in tags_data.get("primary_tags", []):
                if pt["name"] == "Others":
                    pt["subtags"].append({"name": new_subtag, "description": new_subtag})
                    break
    
    # è½¬æ¢ä¸ºè¾“å‡ºæ ¼å¼
    result = []
    for primary, subs in primary_to_subtags.items():
        result.append({
            "name": primary,
            "subtags": subs
        })
    
    return result


def call_llm_with_retry(prompt: str, config: dict, max_retries: int = MAX_RETRIES) -> str:
    """è°ƒç”¨ LLM APIï¼Œæ”¯æŒé‡è¯•"""
    headers = {
        "Content-Type": "application/json",
        "x-api-key": config["api_key"],
        "anthropic-version": "2023-06-01"
    }
    
    payload = {
        "model": config["model"],
        "max_tokens": 1024,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ]
    }
    
    last_error = None
    
    for attempt in range(max_retries):
        try:
            response = requests.post(
                f"{config['base_url']}/v1/messages",
                headers=headers,
                json=payload,
                timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            response.raise_for_status()
            result = response.json()
            return result.get("content", [{}])[0].get("text", "")
        except requests.exceptions.Timeout as e:
            last_error = f"è¶…æ—¶: {e}"
        except requests.exceptions.ConnectionError as e:
            last_error = f"è¿æ¥é”™è¯¯: {e}"
        except requests.exceptions.RequestException as e:
            last_error = f"è¯·æ±‚é”™è¯¯: {e}"
        except Exception as e:
            last_error = f"æœªçŸ¥é”™è¯¯: {e}"
        
        if attempt < max_retries - 1:
            wait_time = RETRY_DELAY * (attempt + 1)
            print(f"       âš ï¸ ç¬¬ {attempt + 1} æ¬¡å¤±è´¥ ({last_error})ï¼Œ{wait_time}s åé‡è¯•...")
            time.sleep(wait_time)
    
    print(f"       âŒ LLM è°ƒç”¨å¤±è´¥ ({max_retries} æ¬¡é‡è¯•å): {last_error}")
    return ""


def parse_llm_response(response: str) -> tuple:
    """
    è§£æ LLM å“åº”ï¼ˆæ–°æ ¼å¼ï¼šåªè¿”å›äºŒçº§æ ‡ç­¾åˆ—è¡¨ï¼‰
    è¿”å›: (subtagsåˆ—è¡¨, æ˜¯å¦è§£ææˆåŠŸ, é”™è¯¯ä¿¡æ¯)
    """
    if not response:
        return [], False, "å“åº”ä¸ºç©º"
    
    # å°è¯•ç›´æ¥è§£æ
    try:
        data = json.loads(response)
        subtags = data.get("subtags", [])
        if isinstance(subtags, list):
            return subtags, True, None
        else:
            return [], False, "subtags æ ¼å¼ä¸æ­£ç¡®"
    except json.JSONDecodeError:
        pass
    
    # å°è¯•ä»ä»£ç å—ä¸­æå–
    json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
    if json_match:
        try:
            data = json.loads(json_match.group(1))
            subtags = data.get("subtags", [])
            if isinstance(subtags, list):
                return subtags, True, None
        except json.JSONDecodeError:
            pass
    
    # å°è¯•æ‰¾åˆ° JSON å¯¹è±¡ - æ›´å®½æ¾çš„åŒ¹é…
    json_match = re.search(r'\{\s*"subtags"\s*:\s*\[.*?\]\s*\}', response, re.DOTALL)
    if json_match:
        try:
            data = json.loads(json_match.group(0))
            subtags = data.get("subtags", [])
            if isinstance(subtags, list):
                return subtags, True, None
        except json.JSONDecodeError:
            pass
    
    return [], False, "æ— æ³•è§£æ JSON"


def build_prompt(title: str, description: str, tags_data: dict) -> str:
    """æ„å»ºæ‰“æ ‡æç¤ºè¯ï¼ˆæ–°ç‰ˆï¼šåªè¯†åˆ«äºŒçº§æ ‡ç­¾ï¼‰"""
    
    # æ„å»ºäºŒçº§æ ‡ç­¾åˆ—è¡¨ä¾› LLM å‚è€ƒ
    subtag_categories = []
    all_primary_names = set()
    for pt in tags_data.get("primary_tags", []):
        all_primary_names.add(pt["name"])
        if pt["name"] == "Others":
            continue  # ä¸æ˜¾ç¤º Others
        subtags = [st["name"] for st in pt.get("subtags", [])]
        if subtags:
            subtag_categories.append(f"ã€{pt['name']}ã€‘: {', '.join(subtags)}")
    
    subtag_list = "\n".join(subtag_categories)
    primary_names_str = ", ".join(sorted(all_primary_names - {"Others"}))
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªç«å“åˆ†æä¸“å®¶ï¼Œè´Ÿè´£ä¸ºç«å“çš„åŠŸèƒ½æ›´æ–°è¿›è¡Œåˆ†ç±»æ‰“æ ‡ã€‚

## å¯ç”¨çš„äºŒçº§æ ‡ç­¾ï¼ˆæŒ‰ç±»åˆ«åˆ†ç»„ï¼‰

{subtag_list}

## å¾…æ‰“æ ‡çš„åŠŸèƒ½

- **æ ‡é¢˜**: {title}
- **æè¿°**: {description}

## ä»»åŠ¡

é€‰æ‹© 1-2 ä¸ªæœ€å‡†ç¡®çš„äºŒçº§æ ‡ç­¾ã€‚æ ‡ç­¾åº”è¯¥äº’æ–¥ï¼Œä¸è¦é€‰æ‹©é‡å çš„æ ‡ç­¾ã€‚

## âš ï¸ ä¸¥æ ¼è§„åˆ™

### 1. ç¦æ­¢è¿”å›ä¸€çº§æ ‡ç­¾å
ä»¥ä¸‹æ˜¯ä¸€çº§æ ‡ç­¾åï¼Œç»å¯¹ä¸èƒ½ä½œä¸ºç»“æœè¿”å›ï¼š{primary_names_str}

### 2. ä¸¥æ ¼åŒ¹é…åŸåˆ™

**Integration å¿…é¡»æ˜ç¡®æåˆ°æœåŠ¡å**
- åªæœ‰æ˜ç¡®æåˆ° "GitHub"ã€"Supabase"ã€"Stripe" ç­‰æœåŠ¡åæ—¶æ‰èƒ½æ‰“å¯¹åº”æ ‡ç­¾
- "repository push" ä¸ç­‰äº GitHubï¼ˆå¯èƒ½æ˜¯å†…ç½® Git åŠŸèƒ½ï¼‰â†’ æ‰“æ€§èƒ½ç›¸å…³æ ‡ç­¾
- "push timing" / "performance" â†’ "Speed"ï¼ˆå±äº Performanceï¼‰

**Backend vs Agent**
- å­˜å‚¨ã€æ•°æ®åº“ç›¸å…³ â†’ "Storage" æˆ– "Database"ï¼ˆå±äº Backendï¼‰
- åªæœ‰æ¶‰åŠ AI è‡ªåŠ¨åŒ–å·¥ä½œæµæ‰æ‰“ Agent æ ‡ç­¾
- "AI Integration Persistence"ï¼ˆå­˜å‚¨ AI ç”Ÿæˆå†…å®¹ï¼‰â†’ "Storage"ï¼Œä¸æ˜¯ Automation

**Social Share vs Integration**
- Twitter/LinkedIn/Telegram **åˆ†äº«æŒ‰é’®** â†’ "Social Share"ï¼ˆå±äº Communityï¼‰
- åªæœ‰çœŸæ­£è°ƒç”¨ API æ‰æ˜¯ Integration

**Social Login vs Integration**  
- Google/Apple/GitHub/Twitter **ç™»å½•** â†’ "Social Login"ï¼ˆå±äº Authï¼‰
- Google Analytics â†’ "Usage Stats"ï¼ˆå±äº Analyticsï¼‰

**Backend vs Integration**
- äº§å“å†…ç½®åç«¯ï¼ˆYouBase/Lovable Cloud/Bolt Databaseï¼‰â†’ Backend
- æ˜ç¡®æåˆ°ç¬¬ä¸‰æ–¹æœåŠ¡åï¼ˆSupabase/Firebaseï¼‰â†’ Integration

### 3. AI Model æ‰“æ ‡
- GPT-4, GPT-5, o1, o3, Codex â†’ "OpenAI"
- Claude Opus, Sonnet, Haiku â†’ "Anthropic"
- Gemini, Veo, Imagen â†’ "Google"ï¼ˆæ¨¡å‹æ›´æ–°ï¼Œä¸æ˜¯ Google ç™»å½•ï¼ï¼‰
- Grok â†’ "xAI"
- Kimi â†’ "Moonshot"
- MiniMax M2 â†’ "MiniMax"
- GLM 4.5, 4.6, 4.7 â†’ "GLM"

### 3.5 Media æ‰“æ ‡ä¸¥æ ¼è§„åˆ™
**Audio Generation åªç”¨äº AI ç”Ÿæˆè¯­éŸ³/éŸ³é¢‘**
- TTSï¼ˆæ–‡å­—è½¬è¯­éŸ³ï¼‰ã€AI é…éŸ³ã€ElevenLabs ç­‰ â†’ "Audio Generation"
- éŸ³é¢‘æ–‡ä»¶ä¸Šä¼ /æ”¯æŒ â†’ "File Upload"ï¼ˆå±äº Fileï¼‰ï¼Œä¸æ˜¯ Audio Generationï¼
- è§†é¢‘ç†è§£ï¼ˆvideo understanding/analysisï¼‰â†’ "Video Understanding"ï¼Œä¸æ˜¯ Audio Generationï¼
- å³ä½¿æè¿°ä¸­æåˆ° "audio understanding"ï¼Œå¦‚æœæ˜¯è§†é¢‘åˆ†æåŠŸèƒ½ â†’ ä»æ˜¯ "Video Understanding"

**Image/Video åŒºåˆ†**
- å›¾ç‰‡ç”Ÿæˆ â†’ "Image Generation"
- å›¾ç‰‡ç¼–è¾‘ â†’ "Image Edit"  
- è§†é¢‘ç”Ÿæˆ â†’ "Video Generation"
- è§†é¢‘åˆ†æ/ç†è§£ â†’ "Video Understanding"

### 3.6 ç¬¬ä¸‰æ–¹æœåŠ¡è¯†åˆ«
**ä»¥ä¸‹æ˜¯ç¬¬ä¸‰æ–¹æœåŠ¡ï¼Œåº”æ‰“ Integration æ ‡ç­¾ï¼ˆéœ€æ˜ç¡®æåˆ°åç§°ï¼‰**ï¼š
- ä»£ç æ‰˜ç®¡: GitHub, GitLab, Bitbucket
- é¡¹ç›®ç®¡ç†: Jira, Linear, Notion, Confluence, Todoist
- é€šè®¯: Slack, Discord, Twilio
- æ”¯ä»˜: Stripe, Plaid
- äº‘æœåŠ¡: Snowflake, AWS, GCP, Azure, Cloudflare
- AI æœåŠ¡: ChatGPT, Perplexity, ElevenLabs, Replicate
- å®¢æœ: Zendesk, Intercom
- è®¾è®¡: Figma
- å¼€å‘å·¥å…·: VS Code, Cursor

**ä»¥ä¸‹ä¸æ˜¯ Integration**ï¼š
- äº§å“å†…ç½®çš„æ•°æ®åº“/å­˜å‚¨ â†’ Backendï¼ˆDatabase/Storageï¼‰
- åˆ†äº«æŒ‰é’® â†’ Communityï¼ˆSocial Shareï¼‰
- ç™»å½•æ–¹å¼ â†’ Authï¼ˆSocial Loginï¼‰

### 4. æ ‡ç­¾äº’æ–¥åŸåˆ™
- æ¯ä¸ªåŠŸèƒ½åªé€‰æœ€å‡†ç¡®çš„ 1-2 ä¸ªæ ‡ç­¾
- é¿å…é€‰æ‹©è¯­ä¹‰é‡å çš„æ ‡ç­¾

### 4.5 ç§»åŠ¨ç«¯ App æ ‡ç­¾åŒºåˆ†
**iOS App / Android App = ç”Ÿæˆç§»åŠ¨åº”ç”¨çš„èƒ½åŠ›**
- åªæœ‰äº§å“æ”¯æŒ"å¯¼å‡º/ç”Ÿæˆ iOS App"æˆ–"å¯¼å‡º/ç”Ÿæˆ Android App"æ—¶æ‰æ‰“è¿™ä¸ªæ ‡ç­¾
- äº§å“æœ¬èº«æœ‰ç§»åŠ¨ç«¯ç‰ˆæœ¬ï¼ˆå¦‚ "YouWare Mobile App"ï¼‰ä¸ç®— â†’ åº”æ‰“ "Mobile Editor" æˆ–å…¶ä»–ç›¸å…³æ ‡ç­¾
- ç§»åŠ¨ç«¯ç¼–è¾‘å™¨åŠŸèƒ½ â†’ "Mobile Editor"ï¼ˆå±äº Editorï¼‰
- ç§»åŠ¨ç«¯æ¨é€é€šçŸ¥ â†’ "Push Notification"ï¼ˆå±äº Communityï¼‰

### 4.6 YouWare ä¸æ”¯æŒçš„åŠŸèƒ½ï¼ˆè¯·å‹¿é”™æ‰“ï¼‰
**ä»¥ä¸‹æ˜¯ YouWare æ˜ç¡®æ²¡æœ‰çš„åŠŸèƒ½ï¼Œä¸è¦ç»™ YouWare æ‰“è¿™äº›æ ‡ç­¾**ï¼š
- Security Scanï¼ˆå®‰å…¨æ‰«æï¼‰ï¼šé™¤éæ˜ç¡®æåˆ°å®‰å…¨æ¼æ´æ‰«æåŠŸèƒ½
- Content Moderationï¼ˆå†…å®¹å®¡æ ¸ï¼‰ï¼šé™¤éæ˜ç¡®æåˆ° AI å†…å®¹å®¡æ ¸ç³»ç»Ÿ
- Keyboard Shortcutsï¼ˆé”®ç›˜å¿«æ·é”®ï¼‰ï¼šé™¤éæ˜ç¡®æåˆ°è‡ªå®šä¹‰å¿«æ·é”®åŠŸèƒ½

### 4.7 Auth å½’ç±»
**ç”¨æˆ·å¯ç”¨çš„è®¤è¯æœåŠ¡å½’å…¥ Backend/Auth**
- äº§å“æä¾›ç»™ç”¨æˆ·é¡¹ç›®ä½¿ç”¨çš„ Auth æœåŠ¡ï¼ˆå¦‚ YouBase çš„è®¤è¯ï¼‰â†’ "Database" æˆ– "Auth Related"ï¼ˆå±äº Backendï¼‰
- ç”¨æˆ·ç™»å½•äº§å“çš„æ–¹å¼ï¼ˆå¦‚ Google ç™»å½• YouWareï¼‰â†’ "Social Login"ï¼ˆå±äº Authï¼‰

### 4.8 Framework æ ‡ç­¾
**å‰ç«¯æ¡†æ¶/åº“æ”¯æŒ**
- TailwindCSSã€shadcnã€Three.jsã€Reactã€Vue â†’ "Framework" ç›¸å…³æ ‡ç­¾
- å¦‚æœæ˜ç¡®æåˆ°æ”¯æŒæŸä¸ªæ¡†æ¶ â†’ æ‰“å¯¹åº”çš„ Framework äºŒçº§æ ‡ç­¾

### 5. Bug ä¿®å¤
- çº¯ç²¹çš„ Bug ä¿®å¤ï¼ˆæ— å…·ä½“åŠŸèƒ½æè¿°ï¼‰â†’ è¿”å›ç©ºæ•°ç»„
- å¦‚æœ Bug ä¿®å¤æ¶‰åŠå…·ä½“åŠŸèƒ½ï¼Œæ‰“å¯¹åº”åŠŸèƒ½çš„æ ‡ç­¾

## è¾“å‡ºæ ¼å¼

```json
{{
    "subtags": ["æ ‡ç­¾1", "æ ‡ç­¾2"]
}}
```

å¦‚æœæ˜¯çº¯ Bug ä¿®å¤æˆ–éåŠŸèƒ½æ€§å†…å®¹ï¼š
```json
{{
    "subtags": []
}}
```

è¯·ç›´æ¥è¾“å‡º JSONï¼š"""
    
    return prompt


def tag_single_feature(title: str, description: str, config: dict, tags_data: dict, subtag_index: dict) -> tuple:
    """
    ä¸ºå•ä¸ªåŠŸèƒ½æ‰“æ ‡
    
    è¿”å›: (tags, success, new_subtags_added)
        - (tags, True, [...]): æˆåŠŸæ‰“æ ‡
        - ([], True, []): LLM åˆ¤æ–­ä¸ºéåŠŸèƒ½æ€§å†…å®¹
        - (None, False, []): è°ƒç”¨å¤±è´¥
    """
    prompt = build_prompt(title, description, tags_data)
    new_subtags_added = []
    
    for attempt in range(MAX_RETRIES):
        response = call_llm_with_retry(prompt, config, max_retries=1)
        
        if not response:
            if attempt < MAX_RETRIES - 1:
                print(f"       âš ï¸ å“åº”ä¸ºç©ºï¼Œé‡è¯• {attempt + 2}/{MAX_RETRIES}...")
                time.sleep(RETRY_DELAY)
                continue
            print(f"       âŒ LLM è°ƒç”¨å¤±è´¥")
            return (None, False, [])
        
        subtags, success, error = parse_llm_response(response)
        
        if success:
            if subtags:
                # è·å–ä¸€çº§æ ‡ç­¾åï¼ˆç”¨äºè¿‡æ»¤ï¼‰
                primary_tag_names = {pt["name"] for pt in tags_data.get("primary_tags", [])}
                existing_subtags = set(subtag_index.get("all_subtags", []))
                
                # è®°å½•æ–°å¢çš„äºŒçº§æ ‡ç­¾ï¼ˆæ’é™¤ä¸€çº§æ ‡ç­¾åï¼‰
                for st in subtags:
                    normalized = normalize_subtag(st, subtag_index)
                    # è·³è¿‡ä¸€çº§æ ‡ç­¾å
                    if normalized in primary_tag_names:
                        continue
                    if normalized not in existing_subtags and st not in existing_subtags:
                        new_subtags_added.append(st)
                
                # æ˜ å°„åˆ°å®Œæ•´æ ‡ç­¾ç»“æ„
                tags = map_subtags_to_tags(subtags, tags_data, subtag_index)
                return (tags, True, new_subtags_added)
            else:
                return ([], True, [])
        
        if not success:
            if attempt < MAX_RETRIES - 1:
                print(f"       âš ï¸ JSON è§£æå¤±è´¥ ({error})ï¼Œé‡è¯• {attempt + 2}/{MAX_RETRIES}...")
                time.sleep(RETRY_DELAY)
                continue
            else:
                print(f"       âŒ JSON è§£æå¤±è´¥: {error}")
                return (None, False, [])
    
    return (None, False, [])


def process_all_features(use_llm: bool = True, limit_per_file: int = None, target_file: str = None):
    """
    å¤„ç†åŠŸèƒ½æ›´æ–°
    
    Args:
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM æ‰“æ ‡
        limit_per_file: æ¯ä¸ªæ–‡ä»¶æœ€å¤šå¤„ç†æ¡æ•°
        target_file: åªå¤„ç†æŒ‡å®šæ–‡ä»¶ (å¦‚: youware.json)
    """
    project_root = get_project_root()
    storage_dir = project_root / "storage"
    
    config = load_config()
    tags_data = load_tags()
    subtag_index = build_subtag_index(tags_data)
    
    total_processed = 0
    total_tagged = 0
    total_skipped = 0
    all_new_subtags = []
    
    # ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if target_file:
        target_path = storage_dir / target_file
        if not target_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {target_file}")
            return
        files_to_process = [target_path]
    else:
        files_to_process = list(storage_dir.glob("*.json"))
    
    for json_file in files_to_process:
        if json_file.name == "example.json":
            continue
        
        print(f"\nå¤„ç† {json_file.name}...")
        
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if len(data) < 2:
            continue
        
        features = data[1].get("features", [])
        
        # æ‰¾å‡ºéœ€è¦æ‰“æ ‡çš„åŠŸèƒ½
        features_to_tag = []
        for i, feat in enumerate(features):
            if "tags" not in feat:
                features_to_tag.append((i, feat))
        
        if limit_per_file:
            features_to_tag = features_to_tag[:limit_per_file]
        
        print(f"  éœ€è¦æ‰“æ ‡: {len(features_to_tag)} æ¡")
        
        tagged_count = 0
        skipped_count = 0
        
        for idx, feat in features_to_tag:
            title = feat.get("title", "")
            description = feat.get("description", "")
            
            # æ˜¾ç¤ºæ›´é•¿çš„æ ‡é¢˜ï¼ˆæœ€å¤š80å­—ç¬¦ï¼‰
            display_title = title[:80] + "..." if len(title) > 80 else title
            print(f"    {total_processed + 1}. {display_title}")
            
            if use_llm:
                tags, success, new_subtags = tag_single_feature(
                    title, description, config, tags_data, subtag_index
                )
                time.sleep(0.5)
            else:
                tags, success, new_subtags = [], True, []
            
            if not success:
                print(f"       â­ï¸ è·³è¿‡ï¼Œç­‰å¾…ä¸‹æ¬¡é‡è¯•")
                total_processed += 1
                continue
            
            if tags:
                features[idx]["tags"] = tags
                tagged_count += 1
                # æ˜¾ç¤ºè¯¦ç»†çš„æ ‡ç­¾ä¿¡æ¯ï¼šä¸€çº§ > äºŒçº§
                tag_details = []
                for t in tags:
                    primary = t["name"]
                    subtag_names = [s["name"] for s in t.get("subtags", [])]
                    if subtag_names:
                        tag_details.append(f"{primary} > {', '.join(subtag_names)}")
                    else:
                        tag_details.append(primary)
                print(f"       âœ“ {' | '.join(tag_details)}")
                
                if new_subtags:
                    print(f"       ğŸ†• æ–°å¢äºŒçº§æ ‡ç­¾ (å½’å…¥ Others): {', '.join(new_subtags)}")
                    all_new_subtags.extend(new_subtags)
                    # ä¿å­˜æ›´æ–°åçš„æ ‡ç­¾ä½“ç³»
                    save_tags(tags_data)
            else:
                features[idx]["tags"] = "None"
                skipped_count += 1
                print(f"       â—‹ éåŠŸèƒ½æ€§å†…å®¹ï¼Œè·³è¿‡")
            
            # æ¯å¤„ç†ä¸€æ¡å°±ç«‹å³ä¿å­˜
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            
            total_processed += 1
        
        print(f"  å·²å¤„ç† {len(features_to_tag)} æ¡ï¼Œæ‰“æ ‡ {tagged_count} æ¡ï¼Œè·³è¿‡ {skipped_count} æ¡")
        total_tagged += tagged_count
        total_skipped += skipped_count
    
    # æœ€ç»ˆä¿å­˜æ ‡ç­¾ä½“ç³»
    if all_new_subtags:
        save_tags(tags_data)
        print(f"\nğŸ“ æ ‡ç­¾ä½“ç³»å·²æ›´æ–°:")
        print(f"   æ–°å¢äºŒçº§æ ‡ç­¾ (å½’å…¥ Others): {', '.join(all_new_subtags)}")
    
    print(f"\n{'='*50}")
    print(f"æ€»è®¡å¤„ç† {total_processed} æ¡åŠŸèƒ½æ›´æ–°")
    print(f"  âœ“ æˆåŠŸæ‰“æ ‡: {total_tagged} æ¡")
    print(f"  â—‹ éåŠŸèƒ½æ€§è·³è¿‡: {total_skipped} æ¡")
    print(f"{'='*50}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="LLM æ‰“æ ‡è„šæœ¬ï¼ˆäºŒçº§æ ‡ç­¾è‡ªåŠ¨æ˜ å°„ä¸€çº§ï¼‰")
    parser.add_argument("--limit", type=int, default=None, help="æ¯ä¸ªæ–‡ä»¶æœ€å¤šå¤„ç†å¤šå°‘æ¡")
    parser.add_argument("--dry-run", action="store_true", help="åªæ˜¾ç¤ºéœ€è¦æ‰“æ ‡çš„æ¡ç›®ï¼Œä¸å®é™…è°ƒç”¨ LLM")
    parser.add_argument("--file", type=str, default=None, help="åªå¤„ç†æŒ‡å®šæ–‡ä»¶ (å¦‚: youware.json)")
    
    args = parser.parse_args()
    
    print("=" * 50)
    print("LLM åŠŸèƒ½æ›´æ–°æ‰“æ ‡ï¼ˆäºŒçº§æ ‡ç­¾ â†’ è‡ªåŠ¨æ˜ å°„ä¸€çº§ï¼‰")
    print(f"é‡è¯•é…ç½®: æœ€å¤š {MAX_RETRIES} æ¬¡, é—´éš” {RETRY_DELAY}s")
    if args.file:
        print(f"å¤„ç†æ–‡ä»¶: {args.file}")
    print("=" * 50)
    
    if args.dry_run:
        process_all_features(use_llm=False, limit_per_file=args.limit, target_file=args.file)
    else:
        process_all_features(use_llm=True, limit_per_file=args.limit, target_file=args.file)


if __name__ == "__main__":
    main()

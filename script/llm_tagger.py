#!/usr/bin/env python3
"""
ä½¿ç”¨ LLM ä¸ºåŠŸèƒ½æ›´æ–°æ‰“æ ‡
è‡ªåŠ¨æ›´æ–° tag.json ä¸­ä¸å­˜åœ¨çš„æ ‡ç­¾
æ”¯æŒç½‘ç»œé”™è¯¯å’Œ JSON è§£æé”™è¯¯é‡è¯•
"""

import json
import time
import re
from pathlib import Path
import requests


# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 2  # ç§’


def load_config():
    """åŠ è½½ LLM é…ç½®"""
    config_path = Path(__file__).parent / "prompts" / "llm_config.json"
    with open(config_path, "r", encoding="utf-8") as f:
        configs = json.load(f)
    return configs[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªé…ç½®


def load_tags():
    """åŠ è½½æ ‡ç­¾ä½“ç³»"""
    tags_path = Path(__file__).parent.parent / "info" / "tag.json"
    with open(tags_path, "r", encoding="utf-8") as f:
        return json.load(f)


def save_tags(tags_data: list):
    """ä¿å­˜æ ‡ç­¾ä½“ç³»"""
    tags_path = Path(__file__).parent.parent / "info" / "tag.json"
    with open(tags_path, "w", encoding="utf-8") as f:
        json.dump(tags_data, f, ensure_ascii=False, indent=4)


def normalize_name(name: str) -> str:
    """æ ‡å‡†åŒ–åç§°ï¼Œç”¨äºæ¨¡ç³ŠåŒ¹é…"""
    return name.lower().strip().replace(" ", "").replace("-", "").replace("_", "")


def get_tag_index(tags_data: list) -> dict:
    """
    æ„å»ºæ ‡ç­¾ç´¢å¼•ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
    åŒ…å«æ ‡å‡†åŒ–åç§°ç”¨äºæ¨¡ç³ŠåŒ¹é…
    """
    index = {}
    norm_to_original = {}  # æ ‡å‡†åŒ–åç§° -> åŸå§‹åç§°
    
    for i, tag in enumerate(tags_data):
        tag_name = tag.get("name", "")
        tag_norm = normalize_name(tag_name)
        
        subtags = {}
        subtag_norm_map = {}  # æ ‡å‡†åŒ–åç§° -> åŸå§‹åç§°
        
        for st in tag.get("subtags", []):
            st_name = st.get("name", "")
            st_norm = normalize_name(st_name)
            subtags[st_name] = True
            subtag_norm_map[st_norm] = st_name
        
        index[tag_name] = {
            "index": i,
            "subtags": set(subtags.keys()),
            "subtag_norm_map": subtag_norm_map
        }
        norm_to_original[tag_norm] = tag_name
    
    index["__norm_to_original__"] = norm_to_original
    return index


def normalize_llm_tags(tags: list, tag_index: dict) -> list:
    """
    æ ‡å‡†åŒ– LLM è¿”å›çš„æ ‡ç­¾ï¼Œä¿®æ­£åç§°å·®å¼‚ï¼ˆç©ºæ ¼ã€å¤§å°å†™ç­‰ï¼‰
    """
    norm_to_original = tag_index.get("__norm_to_original__", {})
    normalized_tags = []
    
    for tag_item in tags:
        tag_name = tag_item.get("name", "")
        if not tag_name:
            continue
        
        tag_norm = normalize_name(tag_name)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡ç³ŠåŒ¹é…çš„ä¸»æ ‡ç­¾
        if tag_name not in tag_index and tag_norm in norm_to_original:
            original_name = norm_to_original[tag_norm]
            print(f"       ğŸ”§ æ ‡ç­¾åç§°ä¿®æ­£: \"{tag_name}\" -> \"{original_name}\"")
            tag_name = original_name
        
        # å¤„ç†å­æ ‡ç­¾
        subtags = tag_item.get("subtags", [])
        normalized_subtags = []
        
        if tag_name in tag_index:
            subtag_norm_map = tag_index[tag_name].get("subtag_norm_map", {})
            
            for st in subtags:
                st_name = st.get("name", "")
                if not st_name:
                    continue
                
                st_norm = normalize_name(st_name)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡ç³ŠåŒ¹é…çš„å­æ ‡ç­¾
                if st_name not in tag_index[tag_name]["subtags"] and st_norm in subtag_norm_map:
                    original_st_name = subtag_norm_map[st_norm]
                    print(f"       ğŸ”§ å­æ ‡ç­¾åç§°ä¿®æ­£: \"{st_name}\" -> \"{original_st_name}\"")
                    st_name = original_st_name
                
                normalized_subtags.append({"name": st_name})
        else:
            # æ–°æ ‡ç­¾ï¼Œä¿æŒåŸæ ·
            normalized_subtags = subtags
        
        normalized_tags.append({
            "name": tag_name,
            "subtags": normalized_subtags
        })
    
    return normalized_tags


def update_tags_with_new(tags_data: list, tag_index: dict, new_tags: list) -> tuple:
    """
    æ£€æŸ¥å¹¶æ›´æ–°æ ‡ç­¾ä½“ç³»
    è¿”å›: (æ˜¯å¦æœ‰æ›´æ–°, æ–°å¢çš„æ ‡ç­¾åˆ—è¡¨, æ–°å¢çš„å­æ ‡ç­¾åˆ—è¡¨)
    """
    updated = False
    new_tag_names = []
    new_subtag_names = []
    norm_to_original = tag_index.get("__norm_to_original__", {})
    
    for tag_item in new_tags:
        tag_name = tag_item.get("name", "")
        subtags = tag_item.get("subtags", [])
        
        if not tag_name:
            continue
        
        tag_norm = normalize_name(tag_name)
        
        # æ£€æŸ¥ä¸»æ ‡ç­¾æ˜¯å¦å­˜åœ¨ï¼ˆåŒ…æ‹¬æ¨¡ç³ŠåŒ¹é…ï¼‰
        if tag_name not in tag_index and tag_name != "__norm_to_original__":
            # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡ç³ŠåŒ¹é…
            if tag_norm in norm_to_original:
                # å·²å­˜åœ¨çš„æ ‡ç­¾ï¼Œè·³è¿‡æ·»åŠ æ–°æ ‡ç­¾
                continue
            
            # æ–°çš„ä¸»æ ‡ç­¾
            new_tag = {
                "name": tag_name,
                "description": f"{tag_name} ç›¸å…³åŠŸèƒ½",
                "subtags": []
            }
            
            subtag_norm_map = {}
            # æ·»åŠ å­æ ‡ç­¾
            for st in subtags:
                st_name = st.get("name", "")
                if st_name:
                    new_tag["subtags"].append({
                        "name": st_name,
                        "description": st_name
                    })
                    subtag_norm_map[normalize_name(st_name)] = st_name
                    new_subtag_names.append(f"{tag_name}/{st_name}")
            
            tags_data.append(new_tag)
            tag_index[tag_name] = {
                "index": len(tags_data) - 1,
                "subtags": {st.get("name", "") for st in subtags if st.get("name")},
                "subtag_norm_map": subtag_norm_map
            }
            # æ›´æ–°æ ‡å‡†åŒ–æ˜ å°„
            norm_to_original[tag_norm] = tag_name
            new_tag_names.append(tag_name)
            updated = True
        elif tag_name in tag_index and tag_name != "__norm_to_original__":
            # ä¸»æ ‡ç­¾å­˜åœ¨ï¼Œæ£€æŸ¥å­æ ‡ç­¾
            existing_subtags = tag_index[tag_name]["subtags"]
            subtag_norm_map = tag_index[tag_name].get("subtag_norm_map", {})
            tag_idx = tag_index[tag_name]["index"]
            
            for st in subtags:
                st_name = st.get("name", "")
                if not st_name:
                    continue
                
                st_norm = normalize_name(st_name)
                
                # æ£€æŸ¥å­æ ‡ç­¾æ˜¯å¦å­˜åœ¨ï¼ˆåŒ…æ‹¬æ¨¡ç³ŠåŒ¹é…ï¼‰
                if st_name not in existing_subtags and st_norm not in subtag_norm_map:
                    # æ–°çš„å­æ ‡ç­¾
                    tags_data[tag_idx]["subtags"].append({
                        "name": st_name,
                        "description": st_name
                    })
                    tag_index[tag_name]["subtags"].add(st_name)
                    subtag_norm_map[st_norm] = st_name
                    new_subtag_names.append(f"{tag_name}/{st_name}")
                    updated = True
    
    return updated, new_tag_names, new_subtag_names


def call_llm_with_retry(prompt: str, config: dict, max_retries: int = MAX_RETRIES) -> str:
    """è°ƒç”¨ LLM APIï¼Œæ”¯æŒé‡è¯•"""
    headers = {
        "Content-Type": "application/json",
        "x-api-key": config["api_key"],
        "anthropic-version": "2023-06-01"
    }
    
    payload = {
        "model": config["model"],
        "max_tokens": 1024,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ]
    }
    
    last_error = None
    
    for attempt in range(max_retries):
        try:
            response = requests.post(
                f"{config['base_url']}/v1/messages",
                headers=headers,
                json=payload,
                timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            response.raise_for_status()
            result = response.json()
            return result.get("content", [{}])[0].get("text", "")
        except requests.exceptions.Timeout as e:
            last_error = f"è¶…æ—¶: {e}"
        except requests.exceptions.ConnectionError as e:
            last_error = f"è¿æ¥é”™è¯¯: {e}"
        except requests.exceptions.RequestException as e:
            last_error = f"è¯·æ±‚é”™è¯¯: {e}"
        except Exception as e:
            last_error = f"æœªçŸ¥é”™è¯¯: {e}"
        
        if attempt < max_retries - 1:
            wait_time = RETRY_DELAY * (attempt + 1)
            print(f"       âš ï¸ ç¬¬ {attempt + 1} æ¬¡å¤±è´¥ ({last_error})ï¼Œ{wait_time}s åé‡è¯•...")
            time.sleep(wait_time)
    
    print(f"       âŒ LLM è°ƒç”¨å¤±è´¥ ({max_retries} æ¬¡é‡è¯•å): {last_error}")
    return ""


def parse_llm_response(response: str) -> tuple:
    """
    è§£æ LLM å“åº”
    è¿”å›: (tagsåˆ—è¡¨, æ˜¯å¦è§£ææˆåŠŸ, é”™è¯¯ä¿¡æ¯)
    """
    if not response:
        return [], False, "å“åº”ä¸ºç©º"
    
    # å°è¯•ç›´æ¥è§£æ
    try:
        data = json.loads(response)
        tags = data.get("tags", [])
        if validate_tags_format(tags):
            return tags, True, None
        else:
            return [], False, "tags æ ¼å¼ä¸æ­£ç¡®"
    except json.JSONDecodeError:
        pass
    
    # å°è¯•ä»ä»£ç å—ä¸­æå–
    json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
    if json_match:
        try:
            data = json.loads(json_match.group(1))
            tags = data.get("tags", [])
            if validate_tags_format(tags):
                return tags, True, None
        except json.JSONDecodeError:
            pass
    
    # å°è¯•æ‰¾åˆ° JSON å¯¹è±¡ - æ›´å®½æ¾çš„åŒ¹é…
    json_match = re.search(r'\{\s*"tags"\s*:\s*\[.*?\]\s*\}', response, re.DOTALL)
    if json_match:
        try:
            data = json.loads(json_match.group(0))
            tags = data.get("tags", [])
            if validate_tags_format(tags):
                return tags, True, None
        except json.JSONDecodeError:
            pass
    
    return [], False, "æ— æ³•è§£æ JSON"


def validate_tags_format(tags: list) -> bool:
    """éªŒè¯ tags æ ¼å¼æ˜¯å¦æ­£ç¡®"""
    if not isinstance(tags, list):
        return False
    
    for tag in tags:
        if not isinstance(tag, dict):
            return False
        if "name" not in tag:
            return False
        if not isinstance(tag.get("name"), str):
            return False
        
        subtags = tag.get("subtags", [])
        if not isinstance(subtags, list):
            return False
        
        for subtag in subtags:
            if not isinstance(subtag, dict):
                return False
            if "name" not in subtag:
                return False
    
    return True


def build_prompt(title: str, description: str, tags_data: list) -> str:
    """æ„å»ºæ‰“æ ‡æç¤ºè¯"""
    tags_json = json.dumps(tags_data, ensure_ascii=False, indent=2)
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªç«å“åˆ†æä¸“å®¶ï¼Œè´Ÿè´£ä¸ºç«å“çš„åŠŸèƒ½æ›´æ–°è¿›è¡Œåˆ†ç±»æ‰“æ ‡ã€‚

## ç°æœ‰æ ‡ç­¾ä½“ç³»

{tags_json}

## å¾…æ‰“æ ‡çš„åŠŸèƒ½

- **æ ‡é¢˜**: {title}
- **æè¿°**: {description}

## ä»»åŠ¡

è¯·ä»ç°æœ‰æ ‡ç­¾ä½“ç³»ä¸­é€‰æ‹©æœ€åˆé€‚çš„æ ‡ç­¾ï¼ˆtagï¼‰å’Œå­æ ‡ç­¾ï¼ˆsubtagï¼‰ã€‚

## è¾“å‡ºè¦æ±‚

ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š

```json
{{
    "tags": [
        {{
            "name": "æ ‡ç­¾åç§°",
            "subtags": [
                {{"name": "å­æ ‡ç­¾1"}},
                {{"name": "å­æ ‡ç­¾2"}}
            ]
        }}
    ]
}}
```

## è§„åˆ™

1. ä¼˜å…ˆä½¿ç”¨ç°æœ‰æ ‡ç­¾å’Œå­æ ‡ç­¾
2. å¯ä»¥é€‰æ‹©å¤šä¸ª tag
3. subtag åº”è¯¥æ˜¯åŠŸèƒ½æ¶‰åŠçš„å…·ä½“ä¸»ä½“ï¼ˆæœåŠ¡åã€æ¨¡å‹åç­‰ï¼‰
4. å¦‚æœç°æœ‰å­æ ‡ç­¾æ²¡æœ‰åŒ¹é…é¡¹ï¼Œå¯ä»¥ç•™ç©º subtags æ•°ç»„
5. å¦‚æœåŠŸèƒ½æ¶‰åŠæ–°çš„å…·ä½“ä¸»ä½“ï¼ˆå¦‚æ–°çš„ç¬¬ä¸‰æ–¹æœåŠ¡ï¼‰ï¼Œå¯ä»¥æ·»åŠ æ–°çš„ subtag

è¯·ç›´æ¥è¾“å‡º JSONï¼š"""
    
    return prompt


def tag_single_feature(title: str, description: str, config: dict, tags_data: list, tag_index: dict) -> tuple:
    """
    ä¸ºå•ä¸ªåŠŸèƒ½æ‰“æ ‡ï¼Œæ”¯æŒé‡è¯•å’Œæ ‡ç­¾åç§°æ ‡å‡†åŒ–
    
    è¿”å›: (tags, success)
        - (tags, True): æˆåŠŸæ‰“æ ‡ï¼Œtags æ˜¯æ ‡ç­¾åˆ—è¡¨
        - ([], True): LLM åˆ¤æ–­ä¸ºéåŠŸèƒ½æ€§å†…å®¹
        - (None, False): è°ƒç”¨å¤±è´¥ï¼Œéœ€è¦ä¸‹æ¬¡é‡è¯•
    """
    prompt = build_prompt(title, description, tags_data)
    
    for attempt in range(MAX_RETRIES):
        # è°ƒç”¨ LLM
        response = call_llm_with_retry(prompt, config, max_retries=1)  # ç½‘ç»œé‡è¯•åœ¨ call_llm_with_retry ä¸­å¤„ç†
        
        if not response:
            if attempt < MAX_RETRIES - 1:
                print(f"       âš ï¸ å“åº”ä¸ºç©ºï¼Œé‡è¯• {attempt + 2}/{MAX_RETRIES}...")
                time.sleep(RETRY_DELAY)
                continue
            print(f"       âŒ LLM è°ƒç”¨å¤±è´¥")
            return (None, False)  # è°ƒç”¨å¤±è´¥
        
        # è§£æå“åº”
        tags, success, error = parse_llm_response(response)
        
        if success:
            if tags:
                # æ ‡å‡†åŒ–æ ‡ç­¾åç§°ï¼ˆä¿®æ­£ç©ºæ ¼ã€å¤§å°å†™å·®å¼‚ï¼‰
                normalized_tags = normalize_llm_tags(tags, tag_index)
                return (normalized_tags, True)
            else:
                # LLM è¿”å›ç©ºæ ‡ç­¾ï¼Œè¯´æ˜æ˜¯éåŠŸèƒ½æ€§å†…å®¹
                return ([], True)
        
        if not success:
            if attempt < MAX_RETRIES - 1:
                print(f"       âš ï¸ JSON è§£æå¤±è´¥ ({error})ï¼Œé‡è¯• {attempt + 2}/{MAX_RETRIES}...")
                time.sleep(RETRY_DELAY)
                continue
            else:
                print(f"       âŒ JSON è§£æå¤±è´¥: {error}")
                return (None, False)  # è§£æå¤±è´¥
    
    return (None, False)  # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥


def process_all_features(use_llm: bool = True, limit_per_file: int = None, target_file: str = None):
    """
    å¤„ç†åŠŸèƒ½æ›´æ–°
    
    Args:
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM æ‰“æ ‡
        limit_per_file: æ¯ä¸ªæ–‡ä»¶æœ€å¤šå¤„ç†æ¡æ•°
        target_file: åªå¤„ç†æŒ‡å®šæ–‡ä»¶ (å¦‚: v0.json)
    """
    project_root = Path(__file__).parent.parent
    storage_dir = project_root / "storage"
    
    config = load_config()
    tags_data = load_tags()
    tag_index = get_tag_index(tags_data)
    
    total_processed = 0
    total_tagged = 0
    total_skipped = 0
    all_new_tags = []
    all_new_subtags = []
    
    # ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if target_file:
        target_path = storage_dir / target_file
        if not target_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {target_file}")
            return
        files_to_process = [target_path]
    else:
        files_to_process = list(storage_dir.glob("*.json"))
    
    for json_file in files_to_process:
        if json_file.name == "example.json":
            continue
        
        print(f"\nå¤„ç† {json_file.name}...")
        
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if len(data) < 2:
            continue
        
        features = data[1].get("features", [])
        
        # æ‰¾å‡ºéœ€è¦æ‰“æ ‡çš„åŠŸèƒ½ï¼ˆtags å­—æ®µä¸å­˜åœ¨çš„æ‰éœ€è¦æ‰“æ ‡ï¼‰
        # tags: "None" è¡¨ç¤ºå·²å¤„ç†è¿‡ä½†åˆ¤å®šä¸ºéåŠŸèƒ½æ€§å†…å®¹ï¼Œä¸éœ€è¦å†å¤„ç†
        # tags: [...] è¡¨ç¤ºå·²æ‰“æ ‡ï¼Œä¸éœ€è¦å†å¤„ç†
        features_to_tag = []
        for i, feat in enumerate(features):
            if "tags" not in feat:
                features_to_tag.append((i, feat))
        
        if limit_per_file:
            features_to_tag = features_to_tag[:limit_per_file]
        
        print(f"  éœ€è¦æ‰“æ ‡: {len(features_to_tag)} æ¡")
        
        tagged_count = 0
        skipped_count = 0
        
        for idx, feat in features_to_tag:
            title = feat.get("title", "")
            description = feat.get("description", "")
            
            print(f"    {total_processed + 1}. {title[:40]}...")
            
            if use_llm:
                tags, success = tag_single_feature(title, description, config, tags_data, tag_index)
                time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
            else:
                tags, success = [], True
            
            if not success:
                # LLM è°ƒç”¨å¤±è´¥ï¼Œä¸è®¾ç½® tagsï¼Œä¸‹æ¬¡ä¼šé‡è¯•
                print(f"       â­ï¸ è·³è¿‡ï¼Œç­‰å¾…ä¸‹æ¬¡é‡è¯•")
                total_processed += 1
                continue
            
            if tags:
                features[idx]["tags"] = tags
                tagged_count += 1
                print(f"       âœ“ {len(tags)} ä¸ªæ ‡ç­¾")
                
                # æ£€æŸ¥å¹¶æ›´æ–°æ ‡ç­¾ä½“ç³»
                updated, new_tags, new_subtags = update_tags_with_new(
                    tags_data, tag_index, tags
                )
                if new_tags:
                    print(f"       ğŸ†• æ–°å¢ä¸»æ ‡ç­¾: {', '.join(new_tags)}")
                    all_new_tags.extend(new_tags)
                if new_subtags:
                    print(f"       ğŸ†• æ–°å¢å­æ ‡ç­¾: {', '.join(new_subtags)}")
                    all_new_subtags.extend(new_subtags)
                
                # æœ‰æ–°æ ‡ç­¾åˆ™ç«‹å³ä¿å­˜æ ‡ç­¾ä½“ç³»
                if updated:
                    save_tags(tags_data)
            else:
                # LLM æˆåŠŸè¿”å›ä½†æ ‡ç­¾ä¸ºç©ºï¼Œè¯´æ˜æ˜¯éåŠŸèƒ½æ€§å†…å®¹
                features[idx]["tags"] = "None"
                skipped_count += 1
                print(f"       â—‹ éåŠŸèƒ½æ€§å†…å®¹ï¼Œè·³è¿‡")
            
            # æ¯å¤„ç†ä¸€æ¡å°±ç«‹å³ä¿å­˜ï¼Œé˜²æ­¢ä¸­æ–­ä¸¢å¤±
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            
            total_processed += 1
        
        print(f"  å·²å¤„ç† {len(features_to_tag)} æ¡ï¼Œæ‰“æ ‡ {tagged_count} æ¡ï¼Œè·³è¿‡ {skipped_count} æ¡")
        total_tagged += tagged_count
        total_skipped += skipped_count
    
    # ä¿å­˜æ›´æ–°åçš„æ ‡ç­¾ä½“ç³»
    if all_new_tags or all_new_subtags:
        save_tags(tags_data)
        print(f"\nğŸ“ æ ‡ç­¾ä½“ç³»å·²æ›´æ–°:")
        if all_new_tags:
            print(f"   æ–°å¢ä¸»æ ‡ç­¾ ({len(all_new_tags)}): {', '.join(all_new_tags)}")
        if all_new_subtags:
            print(f"   æ–°å¢å­æ ‡ç­¾ ({len(all_new_subtags)}): {', '.join(all_new_subtags)}")
    
    print(f"\n{'='*50}")
    print(f"æ€»è®¡å¤„ç† {total_processed} æ¡åŠŸèƒ½æ›´æ–°")
    print(f"  âœ“ æˆåŠŸæ‰“æ ‡: {total_tagged} æ¡")
    print(f"  â—‹ éåŠŸèƒ½æ€§è·³è¿‡: {total_skipped} æ¡")
    print(f"{'='*50}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="LLM æ‰“æ ‡è„šæœ¬")
    parser.add_argument("--limit", type=int, default=None, help="æ¯ä¸ªæ–‡ä»¶æœ€å¤šå¤„ç†å¤šå°‘æ¡")
    parser.add_argument("--dry-run", action="store_true", help="åªæ˜¾ç¤ºéœ€è¦æ‰“æ ‡çš„æ¡ç›®ï¼Œä¸å®é™…è°ƒç”¨ LLM")
    parser.add_argument("--file", type=str, default=None, help="åªå¤„ç†æŒ‡å®šæ–‡ä»¶ (å¦‚: v0.json)")
    
    args = parser.parse_args()
    
    print("=" * 50)
    print("LLM åŠŸèƒ½æ›´æ–°æ‰“æ ‡")
    print(f"é‡è¯•é…ç½®: æœ€å¤š {MAX_RETRIES} æ¬¡, é—´éš” {RETRY_DELAY}s")
    if args.file:
        print(f"å¤„ç†æ–‡ä»¶: {args.file}")
    print("=" * 50)
    
    if args.dry_run:
        process_all_features(use_llm=False, limit_per_file=args.limit, target_file=args.file)
    else:
        process_all_features(use_llm=True, limit_per_file=args.limit, target_file=args.file)


if __name__ == "__main__":
    main()

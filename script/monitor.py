#!/usr/bin/env python3
"""
ç«å“æ›´æ–°ç›‘æ§è„šæœ¬ï¼ˆå¢é‡æ¨¡å¼ï¼‰

å·¥ä½œåŸç†ï¼š
1. å¤‡ä»½ç°æœ‰æ•°æ®ï¼ˆä¿æŠ¤å·²æœ‰çš„ tagsï¼‰
2. è¿è¡Œçˆ¬è™«è·å–æœ€æ–°æ•°æ®
3. åˆå¹¶æ–°æ—§æ•°æ®ï¼Œä¿ç•™å·²æœ‰çš„ tags
4. åªå¯¹æ–°å¢æ¡ç›®è¿›è¡Œ LLM æ‰“æ ‡
5. å®šæœŸå…¨é‡åŒæ­¥æ£€æŸ¥é˜²æ­¢é—æ¼
"""

import json
import subprocess
import sys
import hashlib
import shutil
from datetime import datetime, timedelta
from pathlib import Path


def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    return Path(__file__).parent.parent


def get_feature_key(feature: dict) -> str:
    """
    ç”ŸæˆåŠŸèƒ½æ¡ç›®çš„å”¯ä¸€æ ‡è¯†
    ä½¿ç”¨ title çš„ hash + time ä½œä¸º key
    """
    title = feature.get("title", "")
    time = feature.get("time", "")
    title_hash = hashlib.md5(title.encode()).hexdigest()[:16]
    return f"{title_hash}_{time}"


def load_storage(product_name: str) -> tuple:
    """
    åŠ è½½äº§å“å­˜å‚¨æ•°æ®
    è¿”å›: (data, features, feature_map)
    feature_map: {key: feature} æ–¹ä¾¿æŸ¥æ‰¾å’Œåˆå¹¶
    """
    storage_path = get_project_root() / "storage" / f"{product_name}.json"

    if not storage_path.exists():
        return None, [], {}

    with open(storage_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    if len(data) < 2:
        return data, [], {}

    features = data[1].get("features", [])

    # æ„å»º feature_map
    feature_map = {}
    for f in features:
        key = get_feature_key(f)
        feature_map[key] = f

    return data, features, feature_map


def save_storage(product_name: str, data: list):
    """ä¿å­˜äº§å“æ•°æ®"""
    storage_path = get_project_root() / "storage" / f"{product_name}.json"
    with open(storage_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


def backup_storage(product_name: str) -> dict:
    """
    å¤‡ä»½äº§å“æ•°æ®ï¼Œè¿”å› feature_map
    è¿™æ ·å³ä½¿çˆ¬è™«è¦†ç›–æ–‡ä»¶ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½æ¢å¤ tags
    """
    _, _, feature_map = load_storage(product_name)
    return feature_map


def merge_features(old_feature_map: dict, new_features: list) -> tuple:
    """
    åˆå¹¶æ–°æ—§åŠŸèƒ½æ•°æ®
    - ä¿ç•™æ—§æ¡ç›®çš„ tags
    - è¯†åˆ«çœŸæ­£çš„æ–°å¢æ¡ç›®

    è¿”å›: (merged_features, new_keys)
    """
    merged = []
    new_keys = set()
    seen_keys = set()

    for feature in new_features:
        key = get_feature_key(feature)

        # é¿å…é‡å¤
        if key in seen_keys:
            continue
        seen_keys.add(key)

        if key in old_feature_map:
            # å·²å­˜åœ¨çš„æ¡ç›®ï¼šä¿ç•™åŸæœ‰çš„ tags
            old_feature = old_feature_map[key]
            old_tags = old_feature.get("tags", [])

            # å¦‚æœæ—§æ¡ç›®æœ‰ tagsï¼Œä¿ç•™å®ƒä»¬
            if old_tags and isinstance(old_tags, list) and len(old_tags) > 0:
                feature["tags"] = old_tags
            # å¦åˆ™æ ‡è®°ä¸ºéœ€è¦æ‰“æ ‡
            elif not feature.get("tags") or len(feature.get("tags", [])) == 0:
                new_keys.add(key)
        else:
            # æ–°æ¡ç›®
            new_keys.add(key)
            if "tags" not in feature:
                feature["tags"] = []

        merged.append(feature)

    return merged, new_keys


def get_latest_date(product_name: str) -> str:
    """è·å–äº§å“æœ€æ–°çš„æ›´æ–°æ—¥æœŸ"""
    _, features, _ = load_storage(product_name)

    if not features:
        return None

    # æ‰¾æœ€æ–°çš„æ—¥æœŸ
    dates = [f.get("time", "") for f in features if f.get("time")]
    if not dates:
        return None

    # æŒ‰æ—¥æœŸæ’åºï¼Œå–æœ€æ–°çš„
    try:
        dates.sort(reverse=True)
        return dates[0]
    except:
        return None


def run_crawler(product_name: str) -> bool:
    """
    è¿è¡Œçˆ¬è™«
    è¿”å›: æ˜¯å¦æˆåŠŸ
    """
    crawler_path = get_project_root() / "script" / "crawl" / f"{product_name}.py"

    if not crawler_path.exists():
        print(f"  âš ï¸ çˆ¬è™«è„šæœ¬ä¸å­˜åœ¨: {crawler_path}")
        return False

    try:
        result = subprocess.run(
            [sys.executable, str(crawler_path)],
            capture_output=True,
            text=True,
            timeout=180
        )

        if result.returncode != 0:
            print(f"  âš ï¸ çˆ¬è™«æ‰§è¡Œå¤±è´¥: {result.stderr[:200]}")
            return False

        return True

    except subprocess.TimeoutExpired:
        print(f"  âš ï¸ çˆ¬è™«æ‰§è¡Œè¶…æ—¶")
        return False
    except Exception as e:
        print(f"  âš ï¸ çˆ¬è™«æ‰§è¡Œå¼‚å¸¸: {e}")
        return False


def run_tagging_for_product(product_name: str) -> bool:
    """ä¸ºæŒ‡å®šäº§å“è¿è¡Œæ‰“æ ‡ï¼ˆåªå¤„ç†æ²¡æœ‰ tags çš„æ¡ç›®ï¼‰"""
    tag_script = get_project_root() / "script" / "llm_tagger.py"

    if not tag_script.exists():
        print("  âš ï¸ æ‰“æ ‡è„šæœ¬ä¸å­˜åœ¨")
        return False

    try:
        result = subprocess.run(
            [sys.executable, str(tag_script), "--file", f"{product_name}.json"],
            capture_output=True,
            text=True,
            timeout=600  # æ‰“æ ‡å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
        )
        if result.stdout:
            # åªæ‰“å°å…³é”®ä¿¡æ¯
            for line in result.stdout.split('\n'):
                if 'ğŸ·ï¸' in line or 'âœ“' in line or 'æ–°å¢' in line:
                    print(f"     {line}")
        return result.returncode == 0
    except Exception as e:
        print(f"  âš ï¸ æ‰“æ ‡æ‰§è¡Œå¼‚å¸¸: {e}")
        return False


def load_competitors() -> list:
    """åŠ è½½ç«å“é…ç½®"""
    config_path = get_project_root() / "info" / "competitor.json"

    with open(config_path, "r", encoding="utf-8") as f:
        return json.load(f)


def load_sync_status() -> dict:
    """åŠ è½½åŒæ­¥çŠ¶æ€"""
    status_path = get_project_root() / "info" / "sync_status.json"

    if not status_path.exists():
        return {}

    with open(status_path, "r", encoding="utf-8") as f:
        return json.load(f)


def save_sync_status(status: dict):
    """ä¿å­˜åŒæ­¥çŠ¶æ€"""
    status_path = get_project_root() / "info" / "sync_status.json"

    with open(status_path, "w", encoding="utf-8") as f:
        json.dump(status, f, ensure_ascii=False, indent=4)


def save_update_log(updates: dict):
    """ä¿å­˜æ›´æ–°æ—¥å¿—"""
    log_dir = get_project_root() / "logs"
    log_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = log_dir / f"update_{timestamp}.json"

    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(updates, f, ensure_ascii=False, indent=4)

    print(f"\nğŸ“„ æ›´æ–°æ—¥å¿—å·²ä¿å­˜åˆ°: {log_path}")


def monitor_product(name: str, url: str, force_full: bool = False) -> dict:
    """
    ç›‘æ§å•ä¸ªäº§å“

    æµç¨‹ï¼š
    1. å¤‡ä»½ç°æœ‰æ•°æ®ï¼ˆä¿å­˜ feature_mapï¼‰
    2. è¿è¡Œçˆ¬è™«ï¼ˆçˆ¬è™«ä¼šè¦†ç›–æ–‡ä»¶ï¼‰
    3. åˆå¹¶æ–°æ—§æ•°æ®ï¼Œä¿ç•™å·²æœ‰çš„ tags
    4. åªå¯¹æ–°å¢æ¡ç›®æ‰“æ ‡
    """
    print(f"\nğŸ“¦ {name}")
    print(f"   URL: {url}")

    # 1. å¤‡ä»½ç°æœ‰æ•°æ®
    old_data, old_features, old_feature_map = load_storage(name)
    old_count = len(old_features)
    latest_date = get_latest_date(name)

    print(f"   å·²æœ‰: {old_count} æ¡")
    if latest_date:
        print(f"   æœ€æ–°: {latest_date}")

    # 2. è¿è¡Œçˆ¬è™«
    print(f"   æ­£åœ¨çˆ¬å–...")
    crawler_success = run_crawler(name)

    if not crawler_success:
        print(f"   âŒ çˆ¬è™«å¤±è´¥ï¼Œä¿ç•™åŸæ•°æ®")
        return {
            "status": "crawler_failed",
            "old_count": old_count,
            "new_count": 0
        }

    # 3. åŠ è½½çˆ¬è™«çˆ¬å–çš„æ–°æ•°æ®
    new_data, new_features, _ = load_storage(name)

    if not new_features:
        print(f"   âš ï¸ çˆ¬è™«è¿”å›ç©ºæ•°æ®ï¼Œä¿ç•™åŸæ•°æ®")
        # æ¢å¤åŸæ•°æ®
        if old_data:
            save_storage(name, old_data)
        return {
            "status": "empty_result",
            "old_count": old_count,
            "new_count": 0
        }

    # 4. åˆå¹¶æ•°æ®ï¼Œä¿ç•™å·²æœ‰çš„ tags
    merged_features, new_keys = merge_features(old_feature_map, new_features)

    # 5. æ›´æ–°å¹¶ä¿å­˜æ•°æ®
    if new_data and len(new_data) >= 2:
        new_data[1]["features"] = merged_features
        save_storage(name, new_data)

    new_count = len(new_keys)

    result = {
        "status": "success",
        "old_count": old_count,
        "total_count": len(merged_features),
        "new_count": new_count
    }

    if new_count > 0:
        print(f"   ğŸ†• å‘ç° {new_count} æ¡æ–°åŠŸèƒ½")

        # æ˜¾ç¤ºæ–°å¢æ¡ç›®
        for feature in merged_features:
            key = get_feature_key(feature)
            if key in new_keys:
                title = feature.get('title', '')[:50]
                time = feature.get('time', '')
                print(f"      [{time}] {title}...")
                if len([f for f in merged_features if get_feature_key(f) in new_keys]) > 5:
                    remaining = new_count - 5
                    if remaining > 0:
                        print(f"      ... è¿˜æœ‰ {remaining} æ¡")
                    break

        result["new_features"] = [
            {"title": f.get("title", ""), "time": f.get("time", "")}
            for f in merged_features if get_feature_key(f) in new_keys
        ]

        # 6. ä¸ºæ–°å†…å®¹æ‰“æ ‡
        print(f"   ğŸ·ï¸ æ­£åœ¨ä¸ºæ–°å†…å®¹æ‰“æ ‡...")
        run_tagging_for_product(name)
    else:
        print(f"   âœ… æ— æ–°å¢å†…å®¹")

    return result


def monitor_all(force_full: bool = False):
    """ç›‘æ§æ‰€æœ‰ç«å“"""
    print("=" * 60)
    print("ç«å“æ›´æ–°ç›‘æ§")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    competitors = load_competitors()
    sync_status = load_sync_status()

    all_updates = {
        "timestamp": datetime.now().isoformat(),
        "updates": {}
    }

    total_new = 0

    for competitor in competitors:
        name = competitor.get("name", "")
        url = competitor.get("url", "")

        if not name:
            continue

        try:
            result = monitor_product(name, url, force_full)
            all_updates["updates"][name] = result
            total_new += result.get("new_count", 0)

            # æ›´æ–°åŒæ­¥çŠ¶æ€
            sync_status[name] = {
                "last_sync": datetime.now().isoformat(),
                "latest_date": get_latest_date(name)
            }
        except Exception as e:
            print(f"   âŒ ç›‘æ§å¤±è´¥: {e}")
            all_updates["updates"][name] = {
                "status": "failed",
                "error": str(e)
            }

    # ä¿å­˜åŒæ­¥çŠ¶æ€
    save_sync_status(sync_status)

    # ä¿å­˜æ—¥å¿—
    if total_new > 0:
        save_update_log(all_updates)

    print("\n" + "=" * 60)
    print(f"ç›‘æ§å®Œæˆï¼Œå…±å‘ç° {total_new} æ¡æ–°åŠŸèƒ½")
    print("=" * 60)

    return all_updates


def check_full_sync_needed() -> bool:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦å…¨é‡åŒæ­¥ï¼ˆæ¯å‘¨ä¸€æ¬¡ï¼‰"""
    sync_status = load_sync_status()

    last_full_sync = sync_status.get("__last_full_sync__")
    if not last_full_sync:
        return True

    try:
        last_full = datetime.fromisoformat(last_full_sync)
        days_since = (datetime.now() - last_full).days
        return days_since >= 7
    except:
        return True


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="ç«å“æ›´æ–°ç›‘æ§è„šæœ¬")
    parser.add_argument(
        "--product",
        type=str,
        help="åªç›‘æ§æŒ‡å®šäº§å“ (å¦‚: v0, lovable, bolt)"
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="å¼ºåˆ¶å…¨é‡çˆ¬å–ï¼ˆç”¨äºå®šæœŸå®Œæ•´åŒæ­¥ï¼‰"
    )
    parser.add_argument(
        "--auto",
        action="store_true",
        help="è‡ªåŠ¨æ¨¡å¼ï¼šé€šå¸¸å¢é‡æ›´æ–°ï¼Œæ¯å‘¨ä¸€æ¬¡å…¨é‡"
    )

    args = parser.parse_args()

    # è‡ªåŠ¨æ¨¡å¼
    force_full = args.full
    if args.auto and check_full_sync_needed():
        print("âš ï¸ è·ç¦»ä¸Šæ¬¡å…¨é‡åŒæ­¥è¶…è¿‡ 7 å¤©ï¼Œæ‰§è¡Œå…¨é‡åŒæ­¥")
        force_full = True

        # æ›´æ–°å…¨é‡åŒæ­¥æ—¶é—´
        sync_status = load_sync_status()
        sync_status["__last_full_sync__"] = datetime.now().isoformat()
        save_sync_status(sync_status)

    if args.product:
        # ç›‘æ§å•ä¸ªäº§å“
        competitors = load_competitors()
        competitor = next((c for c in competitors if c.get("name") == args.product), None)

        if not competitor:
            print(f"âŒ æœªæ‰¾åˆ°äº§å“: {args.product}")
            return

        monitor_product(args.product, competitor.get("url", ""), force_full)
    else:
        # ç›‘æ§æ‰€æœ‰äº§å“
        monitor_all(force_full)


if __name__ == "__main__":
    main()
